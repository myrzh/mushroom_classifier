# -*- coding: utf-8 -*-
"""network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FFbbEr1qcY5PDcIcQA_tViIs9r8qHpi9

##Импорт библиотек
"""

import numpy as np
import matplotlib.pyplot as plt

import os
import copy
import time
from PIL import Image
from tqdm import tqdm_notebook

import torch
import torch.nn as nn
import torch.optim as optim

import torchvision
from torchvision import datasets, models, transforms

"""##Импорт датасета с грибами"""

# https://getfile.dokpub.com/yandex/get/https://yadi.sk/d/mNgIXBl4SxeNKg

! wget "https://getfile.dokpub.com/yandex/get/https://yadi.sk/d/mNgIXBl4SxeNKg" -O mushroom_archive.zip

!unzip mushroom_archive.zip -d mushroom_archive

! ls

! ls mushroom_archive

! ls mushroom_archive/train

"""##Импорт предобученной Resnet18"""

model = models.resnet18(pretrained=True) # подключаем предобученную модель
model.eval() # метод eval вызывается, чтобы модель не начала заново обучаться, а только давала предсказания
model

# приводим все изображения к единому виду с помощью фиксированных преобразований
resnet_transforms = transforms.Compose([
        transforms.Resize(256), # размер каждой картинки будет приведен к 256*256
        transforms.CenterCrop(224), # у картинки будет вырезан центральный кусок размера 224*224
        transforms.ToTensor(), # картинка из питоновского массива переводится в формат torch.Tensor
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # значения пикселей картинки нормализуются
    ])

"""##Создание выборок в формате PyTorch-датасета"""

train_data = datasets.ImageFolder('./mushroom_archive/train', transform=resnet_transforms)
val_data = datasets.ImageFolder('./mushroom_archive/valid', transform=resnet_transforms)
test_data = datasets.ImageFolder('./mushroom_archive/test', transform=resnet_transforms)

train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)

"""##Дообучение сети на новом датасете

###Заморозка сверточных слоев и замена последнего слоя
"""

model.fc

neurons = 14
model.fc = nn.Linear(512, neurons)
model

len(list(model.children()))

layers_to_freeze = 8
for i, layer in enumerate(model.children()):
    if i < layers_to_freeze + 1:
        for param in layer.parameters():
            param.requires_grad = False

for param in model.parameters():
    print(param.requires_grad)

"""###Обучение сети"""

def train(model, n_epoch=6):
    # выбираем функцию потерь
    loss_fn = torch.nn.CrossEntropyLoss()

    # выбираем алгоритм оптимизации и learning_rate
    learning_rate = 1e-3
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    train_losses = []
    val_accuracies = []
    val_losses = []
    
    # обучаем сеть
    for epoch in tqdm_notebook(range(n_epoch)):
        train_dataiter = iter(train_loader)
        running_loss = 0.0

        model.train(True)
        for i, batch in enumerate(tqdm_notebook(train_dataiter)):
            X_batch, y_batch = batch # так получаем текущий батч
            
            logits = model(X_batch) # forward pass (получение ответов на батч картинок)
            loss = loss_fn(logits, y_batch) # вычисление лосса от выданных сетью ответов и правильных ответов на батч
            loss.backward() # backpropagation (вычисление градиентов)
            optimizer.step() # обновление весов сети
            optimizer.zero_grad() # обнуляем веса
            # выведем текущий loss
            running_loss += loss.item()

            # выведем лосс каждые 50 батчей
            if i % 50 == 49:
                print('[%d, %5d] loss: %.3f' %
                    (epoch + 1, i + 1, running_loss / 49))
                train_losses.append(running_loss / 49)
                running_loss = 0.0


        

        # evaluate on val
        model.train(False)
        val_dataiter = iter(val_loader)


        val_loss_per_epoch = 0
        val_accuracy_per_epoch = 0
        for i, batch in enumerate(tqdm_notebook(val_dataiter)):
            # так получаем текущий батч
            X_batch, y_batch = batch
            with torch.no_grad():
                logits = model(X_batch)
                y_pred = torch.argmax(logits, dim=1)
                val_accuracy_per_epoch += np.mean(y_pred.numpy() == y_batch.numpy())

                val_loss_per_epoch += loss_fn(logits, y_batch)

        val_accuracies.append(val_accuracy_per_epoch / (i + 1))
        val_losses.append(val_loss_per_epoch / (i + 1))

    print('Обучение закончено')
    return model, train_losses, val_losses, val_accuracies

model, train_losses, val_losses, val_accuracies = train(model, n_epoch=5)

"""##Проверка метрики на тестовой выборке"""

def evaluate(model, dataloader, name='test'):
    accuracy = 0.

    dataiter = iter(dataloader)
    for i, batch in enumerate(tqdm_notebook(dataiter)):
        # так получаем текущий батч
        X_batch, y_batch = batch
        with torch.no_grad():
            logits = model(X_batch)
            y_pred = torch.argmax(logits, dim=1)
            accuracy += np.mean(y_pred.numpy() == y_batch.numpy())
    print(f'{name} accuracy is {accuracy / (i + 1)}')
    return accuracy / (i + 1)

test_dataiter = iter(test_loader)
evaluate(model, test_dataiter, 'test')

"""## Сохранение модели"""

torch.save(model, 'mushroom_model.pt')

"""##Загрузка модели из файла"""

model_new = torch.load('mushroom_model.pt')

test_dataiter = iter(test_loader)
evaluate(model_new, test_dataiter, 'test')

!pip install pipreqs

!pipreqs /content/